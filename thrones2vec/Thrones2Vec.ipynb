{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thrones2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Download NLTK tokenizer models (only the first time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sanju/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sanju/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load books from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['got2.txt', 'got4.txt', 'got5.txt', 'got1.txt', 'got3.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/got1.txt',\n",
       " 'data/got2.txt',\n",
       " 'data/got3.txt',\n",
       " 'data/got4.txt',\n",
       " 'data/got5.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine the books into one string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'data/got1.txt'...\n",
      "Corpus is now 1770659 characters long\n",
      "\n",
      "Reading 'data/got2.txt'...\n",
      "Corpus is now 4071041 characters long\n",
      "\n",
      "Reading 'data/got3.txt'...\n",
      "Corpus is now 6391405 characters long\n",
      "\n",
      "Reading 'data/got4.txt'...\n",
      "Corpus is now 8107945 characters long\n",
      "\n",
      "Reading 'data/got5.txt'...\n",
      "Corpus is now 9719485 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the corpus into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128868"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tokenize.punkt.PunktSentenceTokenizer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#rtemove unnnecessary,, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heraldic crest by Virginia Norey.\n",
      "['Heraldic', 'crest', 'by', 'Virginia', 'Norey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-19 00:04:40,522 : INFO : collecting all words and their counts\n",
      "2017-11-19 00:04:40,524 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-19 00:04:40,587 : INFO : PROGRESS: at sentence #10000, processed 140984 words, keeping 10280 word types\n",
      "2017-11-19 00:04:40,646 : INFO : PROGRESS: at sentence #20000, processed 279730 words, keeping 13558 word types\n",
      "2017-11-19 00:04:40,701 : INFO : PROGRESS: at sentence #30000, processed 420336 words, keeping 16598 word types\n",
      "2017-11-19 00:04:40,766 : INFO : PROGRESS: at sentence #40000, processed 556581 words, keeping 18324 word types\n",
      "2017-11-19 00:04:40,826 : INFO : PROGRESS: at sentence #50000, processed 686247 words, keeping 19714 word types\n",
      "2017-11-19 00:04:40,877 : INFO : PROGRESS: at sentence #60000, processed 828497 words, keeping 21672 word types\n",
      "2017-11-19 00:04:40,935 : INFO : PROGRESS: at sentence #70000, processed 973830 words, keeping 23093 word types\n",
      "2017-11-19 00:04:40,994 : INFO : PROGRESS: at sentence #80000, processed 1114967 words, keeping 24252 word types\n",
      "2017-11-19 00:04:41,058 : INFO : PROGRESS: at sentence #90000, processed 1260481 words, keeping 26007 word types\n",
      "2017-11-19 00:04:41,101 : INFO : PROGRESS: at sentence #100000, processed 1393203 words, keeping 26884 word types\n",
      "2017-11-19 00:04:41,157 : INFO : PROGRESS: at sentence #110000, processed 1532150 words, keeping 27809 word types\n",
      "2017-11-19 00:04:41,220 : INFO : PROGRESS: at sentence #120000, processed 1680961 words, keeping 28486 word types\n",
      "2017-11-19 00:04:41,261 : INFO : collected 29026 word types from a corpus of 1818103 raw words and 128868 sentences\n",
      "2017-11-19 00:04:41,262 : INFO : Loading a fresh vocabulary\n",
      "2017-11-19 00:04:41,331 : INFO : min_count=3 retains 17277 unique words (59% of original 29026, drops 11749)\n",
      "2017-11-19 00:04:41,334 : INFO : min_count=3 leaves 1802699 word corpus (99% of original 1818103, drops 15404)\n",
      "2017-11-19 00:04:41,411 : INFO : deleting the raw counts dictionary of 29026 items\n",
      "2017-11-19 00:04:41,413 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-11-19 00:04:41,414 : INFO : downsampling leaves estimated 1404424 word corpus (77.9% of prior 1802699)\n",
      "2017-11-19 00:04:41,415 : INFO : estimated required memory for 17277 words and 300 dimensions: 50103300 bytes\n",
      "2017-11-19 00:04:41,512 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training, this might take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-19 00:06:04,447 : INFO : training model with 4 workers on 17277 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-11-19 00:06:05,548 : INFO : PROGRESS: at 1.85% examples, 120598 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:06,570 : INFO : PROGRESS: at 3.95% examples, 131516 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-19 00:06:07,653 : INFO : PROGRESS: at 5.90% examples, 128164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:08,714 : INFO : PROGRESS: at 8.29% examples, 132602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:09,810 : INFO : PROGRESS: at 10.39% examples, 134293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:10,815 : INFO : PROGRESS: at 12.34% examples, 134999 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:11,884 : INFO : PROGRESS: at 14.42% examples, 135235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:13,041 : INFO : PROGRESS: at 16.70% examples, 134115 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-19 00:06:14,076 : INFO : PROGRESS: at 18.48% examples, 134013 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:15,133 : INFO : PROGRESS: at 20.31% examples, 133712 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:16,200 : INFO : PROGRESS: at 22.29% examples, 133404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:17,247 : INFO : PROGRESS: at 24.42% examples, 133926 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:18,281 : INFO : PROGRESS: at 26.47% examples, 133994 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:19,323 : INFO : PROGRESS: at 28.78% examples, 134961 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:20,444 : INFO : PROGRESS: at 30.48% examples, 133232 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-19 00:06:21,446 : INFO : PROGRESS: at 32.54% examples, 134052 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:22,483 : INFO : PROGRESS: at 34.17% examples, 132710 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:23,585 : INFO : PROGRESS: at 35.91% examples, 131116 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:24,593 : INFO : PROGRESS: at 37.58% examples, 130280 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:25,625 : INFO : PROGRESS: at 39.09% examples, 129366 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:26,667 : INFO : PROGRESS: at 41.18% examples, 130233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:27,756 : INFO : PROGRESS: at 42.97% examples, 129442 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:28,837 : INFO : PROGRESS: at 44.39% examples, 127809 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:29,925 : INFO : PROGRESS: at 45.87% examples, 126310 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:30,931 : INFO : PROGRESS: at 47.66% examples, 125898 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:32,017 : INFO : PROGRESS: at 49.83% examples, 126520 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:33,038 : INFO : PROGRESS: at 51.22% examples, 125523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:34,113 : INFO : PROGRESS: at 52.63% examples, 124385 words/s, in_qsize 8, out_qsize 1\n",
      "2017-11-19 00:06:35,300 : INFO : PROGRESS: at 54.16% examples, 123081 words/s, in_qsize 8, out_qsize 1\n",
      "2017-11-19 00:06:36,374 : INFO : PROGRESS: at 55.55% examples, 121838 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:37,451 : INFO : PROGRESS: at 57.77% examples, 122523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:38,500 : INFO : PROGRESS: at 59.77% examples, 123257 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:39,538 : INFO : PROGRESS: at 61.81% examples, 123787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:40,559 : INFO : PROGRESS: at 63.64% examples, 123702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:41,578 : INFO : PROGRESS: at 65.61% examples, 124056 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:42,595 : INFO : PROGRESS: at 67.26% examples, 123586 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:43,618 : INFO : PROGRESS: at 68.65% examples, 122725 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:44,757 : INFO : PROGRESS: at 70.35% examples, 122325 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:45,889 : INFO : PROGRESS: at 72.08% examples, 121975 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:46,921 : INFO : PROGRESS: at 74.27% examples, 122630 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:47,963 : INFO : PROGRESS: at 76.66% examples, 123240 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:48,991 : INFO : PROGRESS: at 78.35% examples, 123313 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-19 00:06:50,012 : INFO : PROGRESS: at 80.06% examples, 123421 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:51,015 : INFO : PROGRESS: at 81.59% examples, 123084 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:52,079 : INFO : PROGRESS: at 83.73% examples, 123408 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:53,197 : INFO : PROGRESS: at 85.96% examples, 123759 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:54,198 : INFO : PROGRESS: at 88.37% examples, 124379 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:55,264 : INFO : PROGRESS: at 90.04% examples, 124205 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:56,281 : INFO : PROGRESS: at 92.19% examples, 124757 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:57,346 : INFO : PROGRESS: at 94.26% examples, 125013 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-19 00:06:58,418 : INFO : PROGRESS: at 96.00% examples, 124675 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-19 00:06:59,424 : INFO : PROGRESS: at 97.44% examples, 124213 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-19 00:07:00,432 : INFO : PROGRESS: at 99.26% examples, 124444 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-19 00:07:00,729 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-19 00:07:00,750 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-19 00:07:00,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-19 00:07:00,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-19 00:07:00,829 : INFO : training on 9090515 raw words (7022569 effective words) took 56.4s, 124580 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7022569"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(sentences, total_examples = thrones2vec.corpus_count, epochs = thrones2vec.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to file, can be useful later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-19 00:07:00,933 : INFO : saving Word2Vec object under trained/thrones2vec.w2v, separately None\n",
      "2017-11-19 00:07:00,935 : INFO : not storing attribute syn0norm\n",
      "2017-11-19 00:07:00,937 : INFO : not storing attribute cum_table\n",
      "2017-11-19 00:07:01,833 : INFO : saved trained/thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-19 00:07:13,882 : INFO : loading Word2Vec object from trained/thrones2vec.w2v\n",
      "2017-11-19 00:07:14,226 : INFO : loading wv recursively from trained/thrones2vec.w2v.wv.* with mmap=None\n",
      "2017-11-19 00:07:14,227 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-19 00:07:14,228 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-19 00:07:14,229 : INFO : loaded trained/thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore semantic similarities between book characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words closest to the given word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-19 00:07:16,822 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Roose', 0.931117057800293),\n",
       " ('Ramsay', 0.7579537034034729),\n",
       " ('Wyman', 0.7575839757919312),\n",
       " ('Karstark', 0.7542030811309814),\n",
       " ('Rickard', 0.7527644038200378),\n",
       " ('Jonos', 0.7400764226913452),\n",
       " ('Walder', 0.7288681268692017),\n",
       " ('Blackwood', 0.7237808704376221),\n",
       " ('Hornwood', 0.7065834403038025),\n",
       " ('Arnolf', 0.6951899528503418)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.most_similar(\"Bolton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sansa', 0.7465738654136658),\n",
       " ('Brienne', 0.6997377872467041),\n",
       " ('Gendry', 0.6868095397949219),\n",
       " ('Alayne', 0.6788970828056335),\n",
       " ('Marillion', 0.6593056917190552),\n",
       " ('Meera', 0.6563273072242737),\n",
       " ('Denyo', 0.6470311880111694),\n",
       " ('Catelyn', 0.645492672920227),\n",
       " ('Squirrel', 0.6442588567733765),\n",
       " ('Horseface', 0.6431102156639099)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.most_similar(\"Arya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kevan', 0.7068551778793335),\n",
       " ('Lancel', 0.6879087686538696),\n",
       " ('Tyrion', 0.682525634765625),\n",
       " ('Cersei', 0.6738083362579346),\n",
       " ('Bronn', 0.6667170524597168),\n",
       " ('Brienne', 0.6663480997085571),\n",
       " ('Kingslayer', 0.6657470464706421),\n",
       " ('Joff', 0.6590257287025452),\n",
       " ('Loras', 0.6484676599502563),\n",
       " ('Peck', 0.6484065055847168)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.most_similar(\"Jaime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear relationships between word pairs** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = thrones2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Computes A-B+C **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing the Pet Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nymeria is related to Arya, as Shaggydog is related to Rickon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shaggydog'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Nymeria\", \"Arya\", \"Rickon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing the sigils and houses  relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion is related to Lannister, as direwolf is related to Stark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'direwolf'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"lion\", \"Lannister\", \"Stark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caputuring the Places and Houses relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winterfell is related to Stark, as Rock is related to Lannister\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rock'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Winterfell\", \"Stark\", \"Lannister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Mother and Son relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cersei is related to Joffrey, as Catelyn is related to Robb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Catelyn'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Cersei\", \"Joffrey\", \"Robb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing father and daughter relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sansa is related to Eddard, as Cersei is related to Tywin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cersei'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Sansa\", \"Eddard\", \"Tywin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Sister and Brother relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sansa is related to Rickon, as Cersei is related to Tyrion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cersei'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Sansa\", \"Rickon\", \"Tyrion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Brother and Brother relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bran is related to Rickon, as Tyrion is related to Jaime\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tyrion'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Bran\", \"Rickon\", \"Jaime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Though some relations are captured some are really weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jon is related to Ramsay, as Jorah is related to Dany\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jorah'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"Jon\", \"Ramsay\", \"Dany\")\n",
    "## meaning Ramsay friendzoned Jon !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
